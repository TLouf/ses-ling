from __future__ import annotations
from typing import TYPE_CHECKING
import requests
import re

import lxml.etree
import numpy as np
import pandas as pd
try:
    import cld3
    DEFAULT_CLD = 'cld3'
except ModuleNotFoundError:
    DEFAULT_CLD = 'pycld2'
import pycld2

if TYPE_CHECKING:
    from language_tool_python import LanguageTool

WORD_PATT = re.compile(r'\b[^\W\d_]+?\b')

def clean(tweets_df, text_col='text', acc_th=0.9, min_nr_words=4, min_nr_cjk=4,
          lang=None):
    '''
    From a DataFrame of tweets, detects the language of the text contained in
    'text_col' and output the result in new columns. Before calling a langauge
    detector, the text is pre-processed to rid it of hashtags, usernames in
    mentions and urls. Also, after these are removed, if the remaining text
    is strictly less than 'min_nr_words' words long, we don't bother calling the
    language detector, which will be unreliable anyway.
    '''
    try:
        tweets_lang_df = tweets_df.copy()
        # Match anything starting with @ or # followed by one or more word
        # characters, and which is between word boundaries or at the start or end of
        # the string.
        hash_at_pattern = r'(?:^|\B)((@|#)\w+)(?:$|\b)'
        # Match anything containing /t.co/ surrounded by non-whitespace characters
        # and which is between whitespaces or at the start or end of the string. May
        # be better with all http options at the start, here's it's pretty loose.
        url_pattern = r'(?:^|\s)(\S+\/t.co\/\S+)(?:$|\b)'
        regex_filter = re.compile('({})|({})'.format(hash_at_pattern, url_pattern))
        tweets_lang_df['filtered_text'] = tweets_lang_df[text_col].str.replace(
            regex_filter, '')
        foursquare_mask = tweets_lang_df['source'].str.contains('foursquare')
        im_at_mask = tweets_lang_df['text'].str.startswith("I'm at ")
        # These tweets are generated by Foursquare based on people's location, so
        # they don't tell us anything about the user's language.
        tweets_lang_df.loc[foursquare_mask & im_at_mask, 'filtered_text'] = ''
        # Tweets generated by Foursquare can also end with '(@ <location>)', so we
        # get rid of this part which is useless for language detection, as proper
        # nouns just confuse the detector.
        tweets_lang_df.loc[foursquare_mask, 'filtered_text'] = (
            tweets_lang_df.loc[foursquare_mask, 'filtered_text']
                        .str.replace(r'\(@ .*$', '', regex=True)
        )
        # Tweets generated by Instagram can end with ' @ <location>', so we
        # get rid of this part which is useless for language detection, as proper
        # nouns just confuse the detector.
        insta_mask = tweets_lang_df['source'].str.contains('instagram')
        tweets_lang_df.loc[insta_mask, 'filtered_text'] = (
            tweets_lang_df.loc[insta_mask, 'filtered_text']
                        .str.replace(r' @ .*$', '', regex=True)
        )
        # Tweets generated by Path can contain information about the location
        # (after a 'at') and/or the people the user was with (after a 'with'). This
        # metadata is either in parentheses after the core of the tweet, or, if
        # there's is no core text, right at the start without parentheses. Thus we
        # extract only the core text from what's before a '(at' or '(with'.
        path_mask = tweets_lang_df['source'].str.contains('path')
        tweets_lang_df.loc[path_mask, 'filtered_text'] = (
            tweets_lang_df.loc[path_mask, 'filtered_text']
                        .str.extract(r'(.*)(?:\(with|\(at)', expand=False)
        )
        # Extract returns NaN if there's no match, so we need to convert these
        # to the empty string to avoid errors.
        tweets_lang_df.loc[tweets_lang_df['filtered_text'].isnull(),
                        'filtered_text'] = ''
        # A word is defined by the shortest string of letters between two word
        # boundaries, but a letter here is not simply a-z, because we also want to
        # account for non latin alphabets. A letter is then what is neither a digit
        # (\d), nor an underscore, nor a non-word character (\W: punctuation,
        # special characters, emojis...).
        tweets_lang_df['nr_words'] = tweets_lang_df['filtered_text'].str.count(WORD_PATT)
        # We also count the number of Chinese-Japanese-Korean (CJK) characters,
        # because they can be a full word or syllable, and a whole sentence can be
        # written without any space, so the word count is irrelevant.
        cjk_chars_pattern = re.compile(r'[\u4E00-\u9FFF]')
        nr_cjk_chars = tweets_lang_df['filtered_text'].str.count(cjk_chars_pattern)
        long_enough = (tweets_lang_df['nr_words'] >= min_nr_words) | (nr_cjk_chars >= min_nr_cjk)
        if long_enough.any():
            (tweets_lang_df.loc[long_enough, 'cld_lang'],
            tweets_lang_df.loc[long_enough, 'proba']) = zip(
                *tweets_lang_df.loc[long_enough, 'filtered_text'].apply(make_predict))
            mask = tweets_lang_df['proba'] >= acc_th
            if lang is not None:
                lang_mask = tweets_lang_df['cld_lang'] == lang
                mask = mask & lang_mask
            tweets_lang_df = tweets_lang_df.loc[mask]
            tweets_lang_df['filtered_text'] = tweets_lang_df['filtered_text'].str.strip()
        else:
            cols = tweets_lang_df.columns.tolist() + ['cld_lang', 'proba']
            tweets_lang_df = pd.DataFrame(columns=cols)
    except AttributeError:
        cols = tweets_lang_df.columns.tolist() + ['cld_lang', 'proba']
        tweets_lang_df = pd.DataFrame(columns=cols)
    return tweets_lang_df.drop(columns=['source'])


def make_predict(text: str, cld: str = DEFAULT_CLD, langs_agg_dict=None):
    '''
    From a string of text, gets the language detection from one of the CLD
    versions, and unpacks the results in a dictionary.
    '''
    if langs_agg_dict is None:
        langs_agg_dict = {}
    if  cld == 'cld3':
        raw_predict = cld3.get_language(text)
        lang = raw_predict.language
        proba = raw_predict.probability
    elif cld == 'pycld2':
        # Sometimes there remain utf code like \x92, which makes pycld2 return
        # an error, so we skip these tweets (there are very few of them)
        try:
            raw_predict = pycld2.detect(text)
            lang = raw_predict[2][0][1]
            proba = raw_predict[2][0][2] / 100
        except pycld2.error:
            lang = None
            proba = 0
    # The following is for if the lang is to be aggregated with a similar one.
    # For instance, we might like to aggregate to Chinese ('zh') its traditional
    # form ('zh-Hant').
    agg_to_lang = langs_agg_dict.get(lang)
    if agg_to_lang is not None:
        lang = agg_to_lang
    return lang, proba


def count_mistakes(
    user_corpora: pd.DataFrame, language_tool: LanguageTool
) -> pd.DataFrame:
    '''
    From `user_corpora`, which has `user_id` as Index and a 'text' column containing the
    corpus of tweets corresponding to each user, detect with the `language_tool`
    instance the mistakes made by this user, and then store in `user_mistakes` the
    number of times each user made an error corresponding to a given rule and its
    category. 

    Another simpler way to create `user_mistakes` would simply be to iterate and append
    some lists for every match, but this is nicer because it does not duplicate data
    like `user_id`s, which are very long.
    '''
    # start the server here?
    multiindex_dict = {
        # if really want to save disk space, can do:
        # 'levels': [np.arange(user_corpora.index.size), [], []],
        # and the user_id level values will correspond to the index iloc in user_corpora
        'levels': [user_corpora.index.values, [], []],
        'codes': [[], [], []],
        'names': ['user_id', 'category', 'ruleId']
    }
    # Store code correspondance as a dict for each attribute, in the format `attr_value:
    # attr_code`
    code_corr = {attr_name: {} for attr_name in multiindex_dict['names'][1:]}
    for iloc_user, utuple in enumerate(user_corpora.itertuples()):
        # If too much text for single user, split into chunks to make smaller queries
        # to LT server.
        if len(utuple.text) > 2e4: # 5000 words of 4 characters
            matches = []
            sentences_sets = utuple.text.splitlines(keepends=True)
            nr_words_per_tweet = utuple.nr_words / utuple.nr_tweets
            # roughly have 1000 tweets of 5 words
            n_splits = int(len(sentences_sets) // (5000 / nr_words_per_tweet))
            for i in range(n_splits):
                matches.extend(language_tool.check(
                    ''.join(sentences_sets[i::n_splits])
                ))
        else:
            matches = language_tool.check(utuple.text)
        multiindex_dict['codes'][0].extend(len(matches) * [iloc_user])
        for m in matches:
            for i_attr, attr_name in enumerate(code_corr.keys(), start=1):
                attr_value = getattr(m, attr_name)
                # If it's the first time we encounter this attribute's value, add a new
                # item in `code_corr` and append this value to the levels.
                if attr_value not in code_corr[attr_name]:
                    code_corr[attr_name][attr_value] = len(code_corr[attr_name])
                    multiindex_dict['levels'][i_attr].append(attr_value)
                code = code_corr[attr_name][attr_value]
                multiindex_dict['codes'][i_attr].append(code)

    user_mistakes = (
        pd.DataFrame({'count': True}, index=pd.MultiIndex(**multiindex_dict))
         .groupby(multiindex_dict['names'])
         .sum()
    )
    return user_mistakes


def measure_vocab_size(t: str):
    return len(set(re.findall(WORD_PATT, t)))


def yearly_vocab(user_corpora: pd.DataFrame, words_index: pd.Series):
    '''
    Because unique is not parallelisable, have to do it this way. Reduce rank list sizes
    using the 'words_until' column
    `words_index` sorted by decreasing word's freq, 
    '''
    user_words = (
        user_corpora['text'].str.lower()
         .str.findall(WORD_PATT)
         .explode()
         .rename('word')
         .to_frame()
         .groupby(['user_id', 'word'])
         .size()
         .rename('count')
    )
    user_words = user_words.to_frame().join(words_index.rename('rank'))

    nr_words = words_index.size
    first_time = not 'rank_arr' in user_corpora.columns
    list_arr = []
    list_until = []
    for user_id, g in user_words['rank'].groupby('user_id'):

        if first_time:
            start_new_rank = nr_words
            sorted_ranks = np.sort(g.values)

        else:
            user_rank_arr = user_corpora.loc[user_id, 'rank_arr']
            if isinstance(user_rank_arr, list):
                start_new_rank = max(nr_words, user_rank_arr[-1])
                sorted_ranks = np.sort(np.unique(np.append(user_rank_arr, g.values)))
            else:
                start_new_rank = nr_words
                sorted_ranks = np.sort(g.values)

        is_word_only_instance = np.isnan(sorted_ranks)
        sorted_ranks[is_word_only_instance] = (
            is_word_only_instance[is_word_only_instance].cumsum() + start_new_rank
        )
        sorted_ranks = sorted_ranks.astype(int)
        is_consec = np.diff(sorted_ranks) == 1
        consec_until = 0 if is_consec.size == 0 else is_consec.argmin()
        if consec_until == 0:
            all_until = None
            user_rank_arr = sorted_ranks
        else:
            all_until = sorted_ranks[consec_until]
            user_rank_arr = sorted_ranks[consec_until + 1:]
        list_until.append(all_until)
        list_arr.append(list(user_rank_arr))

    idx_users_with_data = user_words.index.levels[0][np.unique(user_words.index.codes[0])]
    user_corpora.loc[idx_users_with_data, 'words_until'] = list_until
    user_corpora.loc[idx_users_with_data, 'rank_arr'] = list_arr
    return user_corpora


def parse_grammar(source):
    print("Parsing...")
    if source.startswith('http'):
        source = requests.get(source).content
        tree = lxml.etree.fromstring(source)
    else:
        tree = lxml.etree.parse(source)

    list_cats = []
    for cat in tree.iterfind('category'):
        cat_dict = {**cat.attrib}
        # cat_dict['rule_groups'] = [
        #     {'id': rule.attrib['id'], 'rules': [{**rule.attrib}]}
        #     for rule in cat.iterfind('rule')
        # ]
        # cat_dict['rule_groups'].extend([
        #     {**rg.attrib, 'rules': [{**r.attrib} for r in rg.iterfind('rule')]}
        #     for rg in cat.iterfind('rulegroup')
        # ])
        cat_dict['rules'] = [
            {**rule.attrib, 'is_rule_group': False}
            for rule in cat.iterfind('.//rule')
            if 'id' in rule.attrib
        ]
        # rulegroups are not actually a level above rules, the rules they contain do not
        # have an ID
        cat_dict['rules'].extend([
            {**rg.attrib, 'is_rule_group': True}
            for rg in cat.iterfind('rulegroup')
        ])
        list_cats.append(cat_dict)

    nr_rules = sum([len(c['rules']) for c in list_cats])
    print(f'Found {len(list_cats)} categories containing {nr_rules} rules.')
    return list_cats


def parse_online_grammar(lc):
    # remain rules written in java () not in grammar.xml...
    # https://github.com/languagetool-org/languagetool/blob/master/languagetool-language-modules/{lc}/src/main/java/org/languagetool/rules/{lc}/*Rule.java.
    #  But almost neer a category in those, so...
    print("Downloading grammar from LT's repo for parsing...")
    url_format = "https://raw.githubusercontent.com/languagetool-org/languagetool/master/languagetool-language-modules/{lc}/src/main/resources/org/languagetool/rules/{lc}/grammar.xml"
    source = url_format.format(lc=lc)
    list_cats = parse_grammar(source)
    lt_cats_dict = {'categories': list_cats, 'source': source}
    return lt_cats_dict


def get_lt_rules(lt_cats_dict):
    lt_rules = (
        pd.DataFrame
         .from_records(lt_cats_dict['categories'], columns=['id', 'rules'])
         .explode('rules')
    )
    lt_rules = (
        pd.DataFrame.from_records(
            lt_rules['rules'].values.tolist(), index=lt_rules.index
        )
         .join(lt_rules['id'].rename('cat_id').drop_duplicates())
         .set_index(['cat_id', 'id'])
         .rename_axis(index={'id': 'rule_id'})
    )

    return lt_rules


def get_lt_categories(lt_cats_dict):
    lt_categories = (
        pd.DataFrame
         .from_records(lt_cats_dict['categories'], exclude=['rules'])
         .set_index('id')
    )
    return lt_categories
