from typing import TYPE_CHECKING
import re
import pandas as pd
try:
    import cld3
    DEFAULT_CLD = 'cld3'
except ModuleNotFoundError:
    DEFAULT_CLD = 'pycld2'
import pycld2

if TYPE_CHECKING:
    from language_tool_python import LanguageTool


def clean(tweets_df, text_col='text', acc_th=0.9, min_nr_words=4, min_nr_cjk=4,
          lang=None):
    '''
    From a DataFrame of tweets, detects the language of the text contained in
    'text_col' and output the result in new columns. Before calling a langauge
    detector, the text is pre-processed to rid it of hashtags, usernames in
    mentions and urls. Also, after these are removed, if the remaining text
    is strictly less than 'min_nr_words' words long, we don't bother calling the
    language detector, which will be unreliable anyway.
    '''
    try:
        tweets_lang_df = tweets_df.copy()
        # Match anything starting with @ or # followed by one or more word
        # characters, and which is between word boundaries or at the start or end of
        # the string.
        hash_at_pattern = r'(?:^|\B)((@|#)\w+)(?:$|\b)'
        # Match anything containing /t.co/ surrounded by non-whitespace characters
        # and which is between whitespaces or at the start or end of the string. May
        # be better with all http options at the start, here's it's pretty loose.
        url_pattern = r'(?:^|\s)(\S+\/t.co\/\S+)(?:$|\b)'
        regex_filter = re.compile('({})|({})'.format(hash_at_pattern, url_pattern))
        tweets_lang_df['filtered_text'] = tweets_lang_df[text_col].str.replace(
            regex_filter, '')
        foursquare_mask = tweets_lang_df['source'].str.contains('foursquare')
        im_at_mask = tweets_lang_df['text'].str.startswith("I'm at ")
        # These tweets are generated by Foursquare based on people's location, so
        # they don't tell us anything about the user's language.
        tweets_lang_df.loc[foursquare_mask & im_at_mask, 'filtered_text'] = ''
        # Tweets generated by Foursquare can also end with '(@ <location>)', so we
        # get rid of this part which is useless for language detection, as proper
        # nouns just confuse the detector.
        tweets_lang_df.loc[foursquare_mask, 'filtered_text'] = (
            tweets_lang_df.loc[foursquare_mask, 'filtered_text']
                        .str.replace(r'\(@ .*$', '', regex=True)
        )
        # Tweets generated by Instagram can end with ' @ <location>', so we
        # get rid of this part which is useless for language detection, as proper
        # nouns just confuse the detector.
        insta_mask = tweets_lang_df['source'].str.contains('instagram')
        tweets_lang_df.loc[insta_mask, 'filtered_text'] = (
            tweets_lang_df.loc[insta_mask, 'filtered_text']
                        .str.replace(r' @ .*$', '', regex=True)
        )
        # Tweets generated by Path can contain information about the location
        # (after a 'at') and/or the people the user was with (after a 'with'). This
        # metadata is either in parentheses after the core of the tweet, or, if
        # there's is no core text, right at the start without parentheses. Thus we
        # extract only the core text from what's before a '(at' or '(with'.
        path_mask = tweets_lang_df['source'].str.contains('path')
        tweets_lang_df.loc[path_mask, 'filtered_text'] = (
            tweets_lang_df.loc[path_mask, 'filtered_text']
                        .str.extract(r'(.*)(?:\(with|\(at)', expand=False)
        )
        # Extract returns NaN if there's no match, so we need to convert these
        # to the empty string to avoid errors.
        tweets_lang_df.loc[tweets_lang_df['filtered_text'].isnull(),
                        'filtered_text'] = ''
        # A word is defined by the shortest string of letters between two word
        # boundaries, but a letter here is not simply a-z, because we also want to
        # account for non latin alphabets. A letter is then what is neither a digit
        # (\d), nor an underscore, nor a non-word character (\W: punctuation,
        # special characters, emojis...).
        word_any_lang_pattern = re.compile(r'\b[^\W\d_]+?\b')
        tweets_lang_df['nr_words'] = tweets_lang_df['filtered_text'].str.count(word_any_lang_pattern)
        # We also count the number of Chinese-Japanese-Korean (CJK) characters,
        # because they can be a full word or syllable, and a whole sentence can be
        # written without any space, so the word count is irrelevant.
        cjk_chars_pattern = re.compile(r'[\u4E00-\u9FFF]')
        nr_cjk_chars = tweets_lang_df['filtered_text'].str.count(cjk_chars_pattern)
        long_enough = (tweets_lang_df['nr_words'] >= min_nr_words) | (nr_cjk_chars >= min_nr_cjk)
        if long_enough.any():
            (tweets_lang_df.loc[long_enough, 'cld_lang'],
            tweets_lang_df.loc[long_enough, 'proba']) = zip(
                *tweets_lang_df.loc[long_enough, 'filtered_text'].apply(make_predict))
            mask = tweets_lang_df['proba'] >= acc_th
            if lang is not None:
                lang_mask = tweets_lang_df['cld_lang'] == lang
                mask = mask & lang_mask
            tweets_lang_df = tweets_lang_df.loc[mask]
            tweets_lang_df['filtered_text'] = tweets_lang_df['filtered_text'].str.strip()
        else:
            cols = tweets_lang_df.columns.tolist() + ['cld_lang', 'proba']
            tweets_lang_df = pd.DataFrame(columns=cols)
    except AttributeError:
        cols = tweets_lang_df.columns.tolist() + ['cld_lang', 'proba']
        tweets_lang_df = pd.DataFrame(columns=cols)
    return tweets_lang_df.drop(columns=['source'])


def make_predict(text: str, cld: str = DEFAULT_CLD, langs_agg_dict=None):
    '''
    From a string of text, gets the language detection from one of the CLD
    versions, and unpacks the results in a dictionary.
    '''
    if langs_agg_dict is None:
        langs_agg_dict = {}
    if  cld == 'cld3':
        raw_predict = cld3.get_language(text)
        lang = raw_predict.language
        proba = raw_predict.probability
    elif cld == 'pycld2':
        # Sometimes there remain utf code like \x92, which makes pycld2 return
        # an error, so we skip these tweets (there are very few of them)
        try:
            raw_predict = pycld2.detect(text)
            lang = raw_predict[2][0][1]
            proba = raw_predict[2][0][2] / 100
        except pycld2.error:
            lang = None
            proba = 0
    # The following is for if the lang is to be aggregated with a similar one.
    # For instance, we might like to aggregate to Chinese ('zh') its traditional
    # form ('zh-Hant').
    agg_to_lang = langs_agg_dict.get(lang)
    if agg_to_lang is not None:
        lang = agg_to_lang
    return lang, proba


def count_mistakes(
    user_corpora: pd.DataFrame, language_tool: LanguageTool
) -> pd.DataFrame:
    '''
    From `user_corpora`, which has `user_id` as Index and a 'text' column containing the
    corpus of tweets corresponding to each user, detect with the `language_tool`
    instance the mistakes made by this user, and then store in `user_mistakes` the
    number of times each user made an error corresponding to a given rule and its
    category. 

    Another simpler way to create `user_mistakes` would simply be to iterate and append
    some lists for every match, but this is nicer because it does not duplicate data
    like `user_id`s, which are very long.
    '''
    # start the server here?
    multiindex_dict = {
        # if really want to save disk space, can do:
        # 'levels': [np.arange(user_corpora.index.size), [], []],
        # and the user_id level values will correspond to the index iloc in user_corpora
        'levels': [user_corpora.index.values, [], []],
        'codes': [[], [], []],
        'names': ['user_id', 'category', 'ruleId']
    }
    # Store code correspondance as a dict for each attribute, in the format `attr_value:
    # attr_code`
    code_corr = {attr_name: {} for attr_name in multiindex_dict['names'][1:]}
    for iloc_user, utuple in enumerate(user_corpora.itertuples()):
        matches = language_tool.check(utuple.text)
        multiindex_dict['codes'][0].extend(len(matches) * [iloc_user])
        for m in matches:
            for i_attr, attr_name in enumerate(code_corr.keys(), start=1):
                attr_value = getattr(m, attr_name)
                # If it's the first time we encounter this attribute's value, add a new
                # item in `code_corr` and append this value to the levels.
                if attr_value not in code_corr[attr_name]:
                    code_corr[attr_name][attr_value] = len(code_corr[attr_name])
                    multiindex_dict['levels'][i_attr].append(attr_value)
                code = code_corr[attr_name][attr_value]
                multiindex_dict['codes'][i_attr].append(code)

    user_mistakes = (
        pd.DataFrame({'count': True}, index=pd.MultiIndex(**multiindex_dict))
         .groupby(multiindex_dict['names'])
         .sum()
    )
    return user_mistakes
